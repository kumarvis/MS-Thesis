Sensor fusion is a process by which data from several different sensors are ``fused'' to compute something more than could be determined by any one sensor alone. Simple embedded devices like smart phones, smart watches encompass wide variety of sensors like camera, \gps, accelerometer etc. In this 
thesis, we have shown how user experience of these devices can be greatly enhanced by providing them capability to process and analysis these data sensor data automatically. Fusing \gps and vision sensor we have designed an algorithm for localization in 3D and showed how these sensor complementary for 
each other. Moving ahead in a different applications we showed how analyzing and fusing the different 
sensor data along with machine learning algorithms can boost the performance of these devices. We tried to 
optimize the power consumption of batteries in smartphones by using the sensor data from accelerometer, touch screen and cpu usage.

Consider a wearable device for localization in 3D which answers the question ``Where Am I ?'' for a given environment. Localization in 3D is an important problem with wide ranging applications from autonomous navigation in robotics to location specific services on wearable and mobile devices. GPS sensors are a commercially viable option for localization, and are ubiquitous in their use, especially in portable devices. With the proliferation of mobile cameras, however, maturing localization algorithms based on computer vision are emerging as a viable alternative. Although both vision and \gps based localization algorithms have many limitations and inaccuracies, there are some interesting complementarities in their success/failure scenarios that justify an investigation into their joint utilization. Such investigations are further justified considering that many of the modern wearable and mobile computing devices come with sensors for both \gps and vision. In this work, we investigate approaches to reinforce \gps localization with vision algorithms and vice versa. Specifically, we show how noisy \gps signals can be rectified by vision based localization of images captured in the vicinity. Alternatively, we also show how \gps readouts might be used to disambiguate images when they are visually similar looking but belong to different places. Finally, we empirically validate our solutions to show that fusing both these approaches can result in a more accurate and reliable localization of videos captured with a Contour action camera,
over a 600 meter long path, over 10 different days.


There is a rapid growth in memory and processing power of mobile devices unfortunately the battery life is still limited in terms of size and capacity. This implies that managing the battery power is paramount in such devices. As long as the battery technology continues its slow pace of improvement, the only viable approach is to reduce the amount of energy required to provide specific services. Two of the most power consuming services on a smartphone are network and wireless data. Though internet connectivity is important, the nature of data transfer does not require uninterrupted service. We take advantage of this fact to cut off power to these modules when it is not required. The challenge is to predict, when the users require these services and when they do not. We log the sensor data and fuse them to make one single feature and using machine learning approaches to intelligently schedule Wi-Fi according to a userâ€™s activity level. Several higher order features from the raw sensor data stream are used to classify
the activity level. Two aspects of the problem are considered, namely the accuracy of estimating the activity level as well as the power required in sensing and estimation. Experimental results on Android based smartphones demonstrate that an active user can get up to $37\%$ increase in battery life without significant effect on the user experience.