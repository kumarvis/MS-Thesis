\relax 
\newlabel{ch:ack}{{}{v}}
\newlabel{ch:abstract}{{}{vi}}
\citation{survey}
\citation{bworld}
\citation{battery}
\citation{bonnifait2001data,koneru2011fuzzy}
\citation{hays2008im2gps}
\citation{carroll2010analysis}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:intro}{{1}{1}}
\citation{smeulders2000content}
\citation{gudivada1995content}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Localization is a process that determine the position of a robot/human pedestrians in a given environment.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:localization}{{1.1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Figure shows a generic model of Content-based image retrieval system. User can fire the query in form of images in result system return the similar images from the database.\relax }}{3}}
\newlabel{fig:cbir}{{1.2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Computer Vision and Machine Learning Concepts}{3}}
\newlabel{sec:cv_ml_task}{{1.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Content based image retrieval}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Two dimensional data points(blue) are projected along the vector $u_1$(red) in order to reduce the dimensionality.\relax }}{4}}
\newlabel{fig:pca_dimen}{{1.3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Classification}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Dimensionality Reduction}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Statement and Contribution}{5}}
\newlabel{sec_prob_statmnt_contrib}{{1.2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Thesis Outline}{5}}
\newlabel{sec_thesis_outline}{{1.3}{5}}
\citation{hotelling}
\citation{prml}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{6}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:chap2}{{2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Machine Learning Concepts}{6}}
\newlabel{sec_machine_learning_tools}{{2.1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Principal Component Analysis}{6}}
\newlabel{subsec_pca}{{2.1.1}{6}}
\newlabel{eq_pca1}{{2.1}{6}}
\newlabel{eq_pca2}{{2.2}{6}}
\citation{prml}
\newlabel{eq_pca3}{{2.3}{7}}
\newlabel{eq_pca4}{{2.4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Support Vector Machine}{7}}
\newlabel{subsec_svm}{{2.1.2}{7}}
\newlabel{eq_svm1}{{2.5}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.1}Kernelized Approaches}{8}}
\newlabel{subsub_kernelized_approaches}{{2.1.2.1}{8}}
\newlabel{eqn:kernel1}{{2.10}{8}}
\newlabel{eqn:kernel2}{{2.11}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.2}Multiclass SVM}{8}}
\newlabel{subsubsection_mc_svm}{{2.1.2.2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Artificial Neural Networks(ANN)}{8}}
\newlabel{subsec:ANN}{{2.1.3}{8}}
\citation{activation_function}
\citation{heaton2008introduction}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.1}Structure of Artificial Neural Networks}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Figure is decepting a general ANN model and different layers.\relax }}{9}}
\newlabel{fig:ann_diag01}{{2.1}{9}}
\citation{lmnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Nearest Neighbors}{10}}
\newlabel{subsec:Nearest Neighbors}{{2.1.4}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Random Walks on Graphs}{10}}
\citation{random_walks}
\citation{cross_validation}
\citation{bosch_best}
\citation{tsai2012bag}
\citation{tsai2012bag}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Evaluation Measures}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Computer Vision Concepts}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Bag Of Words}{11}}
\citation{mikolajczyk2005local,tuytelaars2008local}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The training set is split into $k$ smaller sets and a model is trained using $k-1$ of the folds as training data. The resulting model is validated on the remaining part of the data. The performance measure reported by $k$-fold cross-validation is then the average of the values computed in the loop.\relax }}{12}}
\newlabel{fig:k_fold_cross_validation}{{2.2}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.1}Interest Point Detection}{12}}
\citation{lowe2004distinctive}
\citation{quelhas2007thousand,mikolajczyk2005local,mikolajczyk2005performance,zhang2007local}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Steps for constructing the bag-of-words for image representation. In first and second step finding the interest region and feature extractions are performed. While in third and fourth step clustering of features and histogram representation of the images are done.\relax }}{13}}
\newlabel{fig:bow_flow}{{2.3}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.2}Local Descriptors}{13}}
\citation{jiang2010representations}
\citation{nister2006scalable}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.3}Visual Word Generation/Vector Clustering}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.4}Building Inverted Index}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1.5}Query Handling}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Trifocal Tensor}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Trifocal geometry of three views. $X$ is a $3D$ point viewed from three camera having optical center $C$, $C'$ and $C''$.\relax }}{15}}
\newlabel{fig_trifocal_tensor}{{2.4}{15}}
\newlabel{eq_projective_transformation}{{2.12}{15}}
\citation{hartley2003multiple}
\newlabel{eq_camera_world_point}{{2.13}{16}}
\newlabel{eq_projective_transformation}{{2.14}{16}}
\newlabel{eq_trifocal_relation}{{2.15}{17}}
\citation{maier2010improved}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Accurate Localization by Fusing Images and GPS Signals}{18}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:chap3}{{3}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Histogram of {\sc  g}ps localization error (top row) of a \emph  {stationary} {\sc  g}ps sensor, showing how inaccurate they can be. Bottom row is an example of two images belonging to the approximately same pose. Visual localization is inaccurate here since in one image the object is occluded, while {\sc  g}ps sensors give accurate localization.\relax }}{19}}
\newlabel{fig:example_alias_occlusion_01}{{3.1}{19}}
\citation{lin2013cross,hafez2013visual}
\citation{hays2008im2gps}
\citation{martin2010precise}
\citation{wei2011intelligent}
\citation{cummins2008fab}
\citation{maier2010improved}
\citation{Zamir_2014_CVPR}
\citation{milford2012seqslam,cummins2011appearance}
\citation{milford2012seqslam}
\citation{levinson2007map,wei2011intelligent}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Related Work}{20}}
\newlabel{sec:related_work}{{3.1.1}{20}}
\citation{turcot2009better,knopp2010avoiding}
\citation{Zamir_2014_CVPR}
\newlabel{fig:flow_diag_concept_02}{{3.2a}{21}}
\newlabel{sub@fig:flow_diag_concept_02}{{(a)}{a}}
\newlabel{fig:block_diagram}{{3.2b}{21}}
\newlabel{sub@fig:block_diagram}{{(b)}{b}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  (a) Problem Statement: We want to simultaneously use the noisy {\sc  g}ps signals and erroneous visual localization to generate accurate localization. (b) The block diagram of the proposed method.\relax }}{21}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { }}}{21}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { }}}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Contributions}{21}}
\citation{nister2006scalable}
\citation{Zamir_2014_CVPR}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Use of GPS for Better Visual Localization and Extracting Useful Features}{22}}
\newlabel{sec:gps_localization_feature}{{3.2}{22}}
\newlabel{eq:cal_score_query_img}{{3.1}{22}}
\citation{wu2011multicore}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Original image features (a) vs those features which could be considered useful features (b). Transient objects, occlusions in the foreground and non-distinctive areas of the scenes are found to be without useful features.\relax }}{23}}
\newlabel{fig:all_feature_useful_feature}{{3.3}{23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { }}}{23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { }}}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Improving GPS Signals through Image Retrieval}{23}}
\newlabel{sec:improve_gps_img_retr}{{3.3}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Details of Algorithm}{23}}
\newlabel{sec:detail_denosing_algo}{{3.3.1}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  Flow chart to extract the useful features. For each query image we retrieve top k images using SIFT BoW. If the geographical distance between the query image and the retrieve image is less then a threshold then we find the inliers between tag them as a useful feature.\relax }}{24}}
\newlabel{fig:flow_diag_useful_features}{{3.4}{24}}
\newlabel{eq:mapping_g_l}{{3.2}{24}}
\citation{Zamir_2014_CVPR}
\citation{jing2008visualrank}
\citation{Zamir_2014_CVPR}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.1}Robust Estimation through Random Walks}{25}}
\newlabel{eq:prob_nodes}{{3.3}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.2}Adaptive Damping Factor}{25}}
\newlabel{eq:basic_random_walk}{{3.5}{25}}
\newlabel{eq:damping_factor_random_walk}{{3.6}{26}}
\newlabel{eq:norm_constant_random_walks}{{3.7}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Improving the Localization}{26}}
\newlabel{sec:improve_localization}{{3.4}{26}}
\citation{contour_camera}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Estimation of initial pose\relax }}{27}}
\newlabel{algo:estimation_init_prob}{{1}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Experiments and Results}{27}}
\newlabel{sec:experiments_results}{{3.5}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Plot demonstrating the noisy and refined {\sc  g}ps signals. One can observe refined {\sc  g}ps signal (red) is less random and in zig-zag shape as compare to noisy {\sc  g}ps signal (green). \relax }}{28}}
\newlabel{fig:noisy_refined_plot}{{3.5}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Effect of Useful Features on Visual Localization}{28}}
\newlabel{subsec:useful_features_visual_localization}{{3.5.1}{28}}
\newlabel{eq:per_error_visual_localization}{{3.9}{28}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Effect of useful features on visual localization. There is a significant drop in percentage error in visual localization with useful features. As $d$ increases the increase in number of inliers are very less. Therefore useful features are almost constant for different values of $d$. Hence the drop in $P_e$ error is less. NA=Not Applicable, OF=Original Features, UF=Useful Features as shown in Figure 3.3\hbox {}.\relax }}{29}}
\newlabel{table:useful_feature_comparison}{{3.1}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Refined vs Noisy GPS Signals RMSE Score}{29}}
\citation{Zamir_2014_CVPR}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Mean RMSE comparison of noisy and refined {\sc  g}ps signal. For refined {\sc  g}ps signal RMSE values $\approx $7m.\relax }}{30}}
\newlabel{table:rmse_mean_err_comparison}{{3.2}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Comparison of Denoising using Synthetic Noise}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Effect of Refined GPS Signal On Localization:}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Plot is demonstrating the synthetic noise reduction process and comparison between Random walks and simply taking the mean. It is clear from the graph as contamination percentage increase Mean is not that much efficient comparison to Random walks in order to remove the added noise. \relax }}{31}}
\newlabel{fig:synthetic_noise}{{3.6}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.5}Multi Sensor Localization}{31}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Effect of refined signal on percentage error. With small value of $\delta _d$ there is significant difference in the performance. As $\delta _d$ increases the performance gap is narrowed down.NS = Noisy Signal, RS = Refined Signal.\relax }}{32}}
\newlabel{table:localization_refined_signal}{{3.3}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Comparison of percentage error $P_e$ in localization with $\delta _d$ =7.5m while using the methods i)Bag-of-Words frame work ii)Bag-of-Words frame work + Integration of modules iii)Bag-of-Words frame work + Integration of modules + Sequential Localization.\relax }}{32}}
\newlabel{table:per_err_integrated_bow}{{3.4}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Bar graph visualization of percentage error in localization using different methodologies.\relax }}{33}}
\newlabel{fig:localization_comp}{{3.7}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Summary}{33}}
\newlabel{sec:summary}{{3.6}{33}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Modeling User Activity for Conserving Power on Smartphones}{34}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:chap5}{{4}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{34}}
\citation{survey}
\citation{survey}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Battery level variation on a smartphone with and without Wi-Fi enabled (Data transfer: 2KB at 5 minute intervals).\relax }}{35}}
\newlabel{fig:variations}{{4.1}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces World wide percentage of mobile phone users and prediction, who use internet or email on their phone in $2014-2019$\cite  {survey}.\relax }}{36}}
\newlabel{fig:bar}{{4.2}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Contributions}{36}}
\citation{Sun}
\citation{bao,lester,ravi,maurer,lester2,wu,rai,weiss}
\citation{longstaff}
\citation{elder}
\citation{yan}
\citation{location}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Related Work}{37}}
\citation{bworld}
\citation{bworld}
\citation{battery}
\citation{battery}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}What consumes Power?}{38}}
\newlabel{sec:WCP}{{4.2}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Component-wise energy consumption on Android (Jeff Sharkey\nobreakspace  {}\cite  {bworld}).\relax }}{38}}
\newlabel{fig:chart}{{4.3}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Duration of time spend under different states (high power, low power and idle) by applications under Bundled and Unbundled packets transfer mode (Kumar Rangarajana\nobreakspace  {}\cite  {battery})\relax }}{39}}
\newlabel{fig:pie}{{4.4}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Wi-Fi scheduling as Multi class Classification}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Primary steps in the algorithm with data flow. Feature extraction module extracts the Accelerometer based, usage based and interaction based features. Classification module predicts the user activity level using SVM Classifier and Wi-Fi Scheduling Module schedules the Wi-Fi state based on the predicted output.\relax }}{40}}
\newlabel{fig:flow}{{4.5}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Classification Frame Work}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Feature Extraction}{40}}
\newlabel{sec:Features}{{4.4.1}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1.1}Accelerometer Based Features}{41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1.2}System Usage Based Features}{42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1.3}Interaction Based Features}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Training and Classification}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2.1}Training}{43}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Class weights assigned to different activity level while training.\relax }}{43}}
\newlabel{tab:weights}{{4.1}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2.2}Classification}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Experimentation and Results}{43}}
\newlabel{sec:Exp}{{4.5}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Implementation}{44}}
\newlabel{sec:Implementation}{{4.5.1}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Power saved by the software}{44}}
\newlabel{powersaved}{{4.5.2}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Energy Efficiency Testing: Percentage battery dropped in Samsung Galaxy S (Dev.1) and HTC Explorer (Dev.2) during controlled tests.\relax }}{45}}
\newlabel{tab:test}{{4.2}{45}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces User Happiness: Count of Number of times Wi-Fi was manually turned on.\relax }}{45}}
\newlabel{tab:user_happiness}{{4.3}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Percentage drop in battery with time under different settings (Dev.1 and Dev.2).\relax }}{46}}
\newlabel{fig:bardrop}{{4.6}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Measuring User Happiness}{46}}
\newlabel{userhapiness}{{4.5.3}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Experiment Extended}{46}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Specifications of the phones on which testing was conducted.\relax }}{47}}
\newlabel{tab:phonespec}{{4.4}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Dataset Collection and Hyperparameter Tuning}{47}}
\citation{scite_learn_framework}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1.1}\textit  {k}NN Paramaeter Tuning}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Tuning number of neigbours(\textit  {k}). We vary \textit  {k} from 1 to 30 and report the mean error on the validation set. We can see after \textit  {k} = 16 the error rate is almost constant which indiactes absence of noise in the model.\relax }}{48}}
\newlabel{fig:knn_err}{{4.7}{48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1.2}SVM Parameter Tuning}{48}}
\citation{scite_learn_framework}
\citation{hilton_web_archive}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Grid search on the dataset to find the optimal C and Gamma pair for the SVM model.\relax }}{49}}
\newlabel{fig:svm_param_tuning}{{4.8}{49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1.3}Neural Networks Paramaeter Tuning}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Summary}{49}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces User Happiness: Count of Number of times Wi-Fi was manually turned on.\relax }}{50}}
\newlabel{tab:no_hdn_layer}{{4.5}{50}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions and future work}{51}}
\@writefile{lof}{\addvspace {1em}}
\@writefile{lot}{\addvspace {1em}}
\newlabel{ch:conc}{{5}{51}}
\bibstyle{unsrt}
\bibdata{sampleBib}
\newlabel{ch:relatedPubs}{{5}{53}}
\bibcite{survey}{1}
\bibcite{bworld}{2}
\bibcite{battery}{3}
\bibcite{bonnifait2001data}{4}
\bibcite{koneru2011fuzzy}{5}
\bibcite{hays2008im2gps}{6}
\bibcite{carroll2010analysis}{7}
\bibcite{smeulders2000content}{8}
\bibcite{gudivada1995content}{9}
\bibcite{hotelling}{10}
\bibcite{prml}{11}
\bibcite{activation_function}{12}
\bibcite{heaton2008introduction}{13}
\@writefile{toc}{{\@tempskipb 3.0ex plus 1pt\relax }}
\@writefile{toc}{\contentsline {chapter}{{Bibliography}}{54}}
\bibcite{lmnn}{14}
\bibcite{random_walks}{15}
\bibcite{cross_validation}{16}
\bibcite{bosch_best}{17}
\bibcite{tsai2012bag}{18}
\bibcite{mikolajczyk2005local}{19}
\bibcite{tuytelaars2008local}{20}
\bibcite{lowe2004distinctive}{21}
\bibcite{quelhas2007thousand}{22}
\bibcite{mikolajczyk2005performance}{23}
\bibcite{zhang2007local}{24}
\bibcite{jiang2010representations}{25}
\bibcite{nister2006scalable}{26}
\bibcite{hartley2003multiple}{27}
\bibcite{maier2010improved}{28}
\bibcite{lin2013cross}{29}
\bibcite{hafez2013visual}{30}
\bibcite{martin2010precise}{31}
\bibcite{wei2011intelligent}{32}
\bibcite{cummins2008fab}{33}
\bibcite{Zamir_2014_CVPR}{34}
\bibcite{milford2012seqslam}{35}
\bibcite{cummins2011appearance}{36}
\bibcite{levinson2007map}{37}
\bibcite{turcot2009better}{38}
\bibcite{knopp2010avoiding}{39}
\bibcite{wu2011multicore}{40}
\bibcite{jing2008visualrank}{41}
\bibcite{contour_camera}{42}
\bibcite{Sun}{43}
\bibcite{bao}{44}
\bibcite{lester}{45}
\bibcite{ravi}{46}
\bibcite{maurer}{47}
\bibcite{lester2}{48}
\bibcite{wu}{49}
\bibcite{rai}{50}
\bibcite{weiss}{51}
\bibcite{longstaff}{52}
\bibcite{elder}{53}
\bibcite{yan}{54}
\bibcite{location}{55}
\bibcite{scite_learn_framework}{56}
\bibcite{hilton_web_archive}{57}
